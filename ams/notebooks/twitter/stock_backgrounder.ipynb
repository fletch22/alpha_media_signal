{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains on historical EOD to act as a single column in twitter dataset.\n",
    "\n",
    "from pathlib import Path\n",
    "from ams.config import constants\n",
    "from ams.pipes.p_make_prediction.mp_process import get_stocks_merged\n",
    "from ams.services.ticker_service import get_ticker_eod_data, make_one_hotted, get_ticker_info\n",
    "import pandas as pd\n",
    "from ams.twitter.twitter_ml_utils import combine_with_quarterly_stock_data\n",
    "from ams.twitter.twitter_ml_utils import merge_with_stock_details\n",
    "\n",
    "twit_root_path = constants.TWITTER_OUTPUT_RAW_PATH\n",
    "src_path = Path(twit_root_path, \"stock_merge_drop\", \"main\")\n",
    "\n",
    "df = get_stocks_merged(stock_merge_drop_path=src_path)\n",
    "\n",
    "tickers = df[\"f22_ticker\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tickers = ['AAI', 'AAL', 'ACNAQ', 'ALGT', 'ASAI', 'CCAR', 'COMR', 'FLYI', 'FRNTQ', 'HA',\n",
    " 'JBLU', 'MAIR', 'MDWY', 'MESA', 'MESAQ', 'PNCLQ', 'RENO', 'RJETQ', 'RYAAY', 'SKYW',\n",
    " 'TOWR1', 'TRIP1', 'UAL', 'VA', 'VIRGY', 'VNGD']\n",
    "\n",
    "max_tickers = 200\n",
    "df_all = []\n",
    "industries = ['Airlines']\n",
    "\n",
    "for ndx, ticker in enumerate(tickers):\n",
    "    df_eod = get_ticker_eod_data(ticker=ticker)\n",
    "    df_all.append(df_eod)\n",
    "\n",
    "    if max_tickers is not None and ndx >= max_tickers:\n",
    "        break\n",
    "\n",
    "df_eod_all = pd.concat(df_all, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:38:00 - ams.twitter.twitter_ml_utils:640 - INFO - Finished merging in quarterly stock data.\n"
     ]
    }
   ],
   "source": [
    "df_w_fundy, _ = combine_with_quarterly_stock_data(df=df_eod_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def merge_stock_data_w_details(df: pd.DataFrame, industries: List[str]):\n",
    "    df_merged = merge_with_stock_details(df)\n",
    "\n",
    "    df_merged.sort_values(by=[\"ticker\", \"date\"], inplace=True)\n",
    "\n",
    "    if df_merged.shape[0] == 0:\n",
    "        logger.info(\"Not enough data after merge.\")\n",
    "        \n",
    "    df_filtered = df_merged[df_merged[\"industry\"].isin(industries)].copy()\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "df_merged = merge_stock_data_w_details(df=df_w_fundy, industries=industries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"ticker\"].unique()\n",
    "df_merged.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_merged.columns)\n",
    "\n",
    "col_drops = []\n",
    "for c in cols:\n",
    "    uniques = list(df_merged[c].unique())\n",
    "    if len(uniques) == 1:\n",
    "        col_drops.append(c)\n",
    "\n",
    "df_dropped = df_merged.drop(columns=col_drops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "col_drops = ['calendardate', 'datekey', 'reportperiod']\n",
    "df_hot_ready = df_dropped.drop(columns=col_drops)\n",
    "\n",
    "df_all_tickers = get_ticker_info()\n",
    "col_objs = [c for c in df_hot_ready.columns if str(df_dropped[c].dtype) == \"object\"]\n",
    "col_objs.remove(\"date\")\n",
    "\n",
    "df_hotted = make_one_hotted(df=df_hot_ready, df_all_tickers=df_all_tickers, cols=col_objs)\n",
    "\n",
    "rem_cols = list(df_hotted.columns)\n",
    "rem_cols.remove(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['calendardate'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-0e86100dc029>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcol_drops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'calendardate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'datekey'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_dropped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_dropped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol_drops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_all_tickers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ticker_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4174\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4175\u001b[0m         )\n\u001b[0;32m   4176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3919\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3920\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3921\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3922\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5283\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5284\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5285\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5286\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['calendardate'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df_col_ordered = df_hotted[rem_cols + [\"stock_val_change\", \"date\"]].copy()\n",
    "\n",
    "df_col_ordered.fillna(0, inplace=True)\n",
    "\n",
    "cols = df_col_ordered.columns\n",
    "\n",
    "# print(list(cols))\n",
    "\n",
    "df_flight = df_col_ordered[cols].copy()\n",
    "\n",
    "# df_flight[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dates = sorted(df_flight[\"date\"].unique())\n",
    "\n",
    "train_frac = 0.7\n",
    "\n",
    "total_size = len(dates)\n",
    "train_size = int(total_size * train_frac)\n",
    "train_dates = dates[:train_size]\n",
    "val_test_dates = dates[-train_size:]\n",
    "\n",
    "test_frac = .50\n",
    "test_size = int(len(val_test_dates) * test_frac)\n",
    "val_dates = val_test_dates[:test_size]\n",
    "test_dates = val_test_dates[-test_size:]\n",
    "\n",
    "df_train = df_flight.loc[df_flight[\"date\"].isin(train_dates)]\n",
    "df_val = df_flight.loc[df_flight[\"date\"].isin(val_dates)]\n",
    "df_test = df_flight.loc[df_flight[\"date\"].isin(test_dates)]\n",
    "\n",
    "df_train = df_train.loc[:, df_train.columns != \"date\"]\n",
    "df_val = df_val.loc[:, df_val.columns != \"date\"]\n",
    "df_test = df_test.loc[:, df_test.columns != \"date\"]\n",
    "\n",
    "print(list(df_test.columns)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self, n_features, seq_length):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "\n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features,\n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers,\n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is\n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers, batch_size, self.n_hidden)\n",
    "        cell_state = torch.zeros(self.n_layers, batch_size, self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "    def forward(self, x, future=0):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        lstm_out, self.hidden = self.l_lstm(x, self.hidden)\n",
    "        # lstm_out(with batch_first = True) is\n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest\n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        return self.l_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = df_train.astype('float32').to_numpy()\n",
    "dataset_val = df_val.astype('float32').to_numpy()\n",
    "dataset_test = df_test.astype('float32').to_numpy()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_arr = scaler.fit_transform(dataset_train)\n",
    "val_arr = scaler.transform(dataset_val)\n",
    "test_arr = scaler.transform(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = len(df_flight.columns) - 2 # this is number of parallel inputs\n",
    "n_timesteps = 60 # this is number of timesteps\n",
    "\n",
    "x_train, y_train = split_sequences(train_arr, n_timesteps)\n",
    "x_val, y_val = split_sequences(val_arr, n_timesteps)\n",
    "x_test, y_test = split_sequences(test_arr, n_timesteps)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, seq_len, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lstm = torch.nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        self.linear = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, future=0, y=None):\n",
    "        outputs = []\n",
    "\n",
    "        # reset the state of LSTM\n",
    "        # the state is kept till the end of the sequence\n",
    "        h_t = torch.zeros(input.size(0), self.hidden_size, dtype=torch.float32)\n",
    "        c_t = torch.zeros(input.size(0), self.hidden_size, dtype=torch.float32)\n",
    "\n",
    "        print(h_t.size())\n",
    "        print(c_t.size())\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "            h_t, c_t = self.lstm(input_t, (h_t, c_t))\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "\n",
    "        for i in range(future):\n",
    "            if y is not None and random.random() > 0.5:\n",
    "                output = y[:, [i]]  # teacher forcing\n",
    "            h_t, c_t = self.lstm(output, (h_t, c_t))\n",
    "            output = self.linear(h_t)\n",
    "            outputs += [output]\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sequence(scaler, model, x_sample, future=1000):\n",
    "    \"\"\" Generate future values for x_sample with the model \"\"\"\n",
    "    y_pred_tensor = model(x_sample, future=future)\n",
    "    y_pred = y_pred_tensor.cpu().tolist()\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def to_dataframe(actual, predicted):\n",
    "    return pd.DataFrame({\"actual\": actual, \"predicted\": predicted})\n",
    "\n",
    "\n",
    "def inverse_transform(scalar, df, columns):\n",
    "    for col in columns:\n",
    "        print(df[col].values.shape)\n",
    "        values = df[col].values.reshape(-1, 1)\n",
    "        df[col] = scaler.inverse_transform(values)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch_data(x, y, batch_size: int):\n",
    "    for batch_ndx, i in enumerate(range(0, len(x) - batch_size, batch_size)):\n",
    "        x_batch = x[i:i + batch_size, :, :]\n",
    "        y_batch = y[i:i + batch_size]\n",
    "\n",
    "        x_batch = torch.tensor(x_batch, dtype=torch.float32)\n",
    "        y_batch = torch.tensor(y_batch, dtype=torch.float32)\n",
    "\n",
    "        yield x_batch, y_batch, batch_ndx\n",
    "\n",
    "class Optimization:\n",
    "    \"\"\" A helper class to train, test and diagnose the LSTM\"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn, optimizer, scheduler):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.futures = []\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val=None,\n",
    "        y_val=None,\n",
    "        batch_size=16,\n",
    "        n_epochs=120,\n",
    "        do_teacher_forcing=None,\n",
    "    ):\n",
    "        seq_len = x_train.shape[1]\n",
    "        self.model.train()\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = 0\n",
    "            start_time = time.time()\n",
    "            for x_batch, y_batch, batch_ndx in generate_batch_data(x_train, y_train, batch_size):\n",
    "\n",
    "                self.model.init_hidden(x_batch.size(0))\n",
    "\n",
    "                y_pred = self._predict(x_batch, y_batch, seq_len, do_teacher_forcing)\n",
    "                loss = self.loss_fn(y_pred.view(-1), y_batch)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= batch_ndx\n",
    "            elapsed = time.time() - start_time\n",
    "            print('step : ' , epoch , 'loss : ' , loss.item())\n",
    "\n",
    "            self._validation(x_val, y_val, batch_size)\n",
    "\n",
    "            print(\n",
    "                \"Epoch %d Train loss: %.8f. Validation loss: %.8f. Avg future: %.2f. Elapsed time: %.2fs.\"\n",
    "                % (epoch + 1, train_loss, self.val_losses[-1], np.average(self.futures), elapsed)\n",
    "            )\n",
    "\n",
    "    def _predict(self, x_batch, y_batch, seq_len, do_teacher_forcing):\n",
    "        if do_teacher_forcing:\n",
    "            future = random.randint(1, int(seq_len) / 2)\n",
    "            limit = x_batch.size(1) - future\n",
    "            y_pred = self.model(x_batch[:, :limit], future=future, y=y_batch[:, limit:])\n",
    "        else:\n",
    "            future = 0\n",
    "            y_pred = self.model(x_batch)\n",
    "        self.futures.append(future)\n",
    "        return y_pred\n",
    "\n",
    "    def _validation(self, x_val, y_val, batch_size):\n",
    "        if x_val is None or y_val is None:\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            batch_ndx = 0\n",
    "            for x_batch, y_batch, batch_ndx in generate_batch_data(x_val, y_val, batch_size):\n",
    "#                 y_pred = self.model(x_batch)\n",
    "#                 loss = self.loss_fn(y_pred, y_batch)\n",
    "#                 val_loss += loss.item()\n",
    "                output = self.model(x_batch)\n",
    "                loss = self.loss_fn(output.view(-1), y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            val_loss /= batch_ndx\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "    def evaluate(self, x_test, y_test, batch_size, future=1):\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            batch_ndx = 0\n",
    "            actual, predicted = [], []\n",
    "            for x_batch, y_batch, batch_ndx in generate_batch_data(x_test, y_test, batch_size):\n",
    "                y_pred = self.model(x_batch, future=future)\n",
    "\n",
    "                y_batch = y_batch.reshape(-1, 1)\n",
    "\n",
    "#                 print(y_pred.shape)\n",
    "#                 print(y_batch.shape)\n",
    "#                 print(y_batch.reshape(-1, 1).shape)\n",
    "#                 print(y_batch.shape[1])\n",
    "#                 print(y_pred[:, -len(y_batch)])\n",
    "#                 print(y_pred)\n",
    "\n",
    "                y_pred = (\n",
    "                    y_pred[:, -len(y_batch)] if y_pred.shape[1] > y_batch.shape[1] else y_pred\n",
    "                )\n",
    "\n",
    "\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                test_loss += loss.item()\n",
    "                actual += torch.squeeze(y_batch[:, -1]).data.cpu().numpy().tolist()\n",
    "                predicted += torch.squeeze(y_pred[:, -1]).data.cpu().numpy().tolist()\n",
    "            test_loss /= batch_ndx\n",
    "            return actual, predicted, test_loss\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_1 = MV_LSTM(n_features, n_timesteps)\n",
    "loss_fn_1 = torch.nn.MSELoss()\n",
    "optimizer_1 = torch.optim.Adam(model_1.parameters(), lr=1e-2)\n",
    "scheduler_1 = optim.lr_scheduler.StepLR(optimizer_1, step_size=5, gamma=0.1)\n",
    "\n",
    "optimization_1 = Optimization(model_1, loss_fn_1, optimizer_1, scheduler_1)\n",
    "\n",
    "optimization_1.train(x_train, y_train, x_val, y_val, do_teacher_forcing=False, n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actual_1, predicted_1, test_loss_1 = optimization_1.evaluate(x_test, y_test, future=1, batch_size=batch_size)\n",
    "df_result_1 = to_dataframe(actual_1, predicted_1)\n",
    "\n",
    "# df_result_1\n",
    "df_result_1 = inverse_transform(scaler, df_result_1, ['actual', 'predicted'])\n",
    "# df_result_1.plot(figsize=(14, 7))\n",
    "# print(\"Test loss %.4f\" % test_loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create NN\n",
    "mv_net = MV_LSTM(n_features, n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-5)\n",
    "\n",
    "train_episodes = 120\n",
    "batch_size = 16\n",
    "\n",
    "mv_net.train()\n",
    "X = x_train\n",
    "y = y_train\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]\n",
    "\n",
    "        x_batch = torch.tensor(inpt, dtype=torch.float32)\n",
    "        y_batch = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)\n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch)\n",
    "        loss = criterion(output.view(-1), y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print('step : ' , t , 'loss : ' , loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
