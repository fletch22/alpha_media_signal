{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Chris\\\\workspaces\\\\alpha_media_signal\\\\ams\\\\notebooks\\\\twitter', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\alpha_media_signal', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\open_source\\\\spark-3.0.1-bin-hadoop2.7\\\\python', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\open_source\\\\spark-3.0.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal\\\\python37.zip', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal\\\\DLLs', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal\\\\lib', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal', '', 'C:\\\\Users\\\\Chris\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages\\\\win32', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\ProgramData\\\\Miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Chris\\\\.ipython', '/home/jovyan/work']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "paths_to_add = ['/home/jovyan/work']\n",
    "\n",
    "for p in paths_to_add:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "print(sys.path)\n",
    "native_spark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ams.services import spark_service\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession, udf\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from ams.services import twitter_service\n",
    "from pyspark.sql import functions as F\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StringType, StructType, StructField, BooleanType, MapType, ArrayType, Row\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import re\n",
    "\n",
    "spark = spark_service.get_or_create(app_name='twitter_flatten')\n",
    "\n",
    "sc = spark.sparkContext\n",
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(__name__)\n",
    "\n",
    "LOGGER.info(\"pyspark script logger initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ams.config import constants\n",
    "\n",
    "if native_spark:\n",
    "    project_root = \"../../../\"\n",
    "    data_path = Path(constants.DATA_PATH)\n",
    "else:\n",
    "    data_path = Path('/home/jovyan/work/data/')\n",
    "    project_root = \"/home/jovyan/work/\"\n",
    "\n",
    "twitter_folder = 'twitter'\n",
    "\n",
    "file_path = Path(data_path, twitter_folder, 'fixed_drop', 'staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ams.services import schema_service\n",
    "\n",
    "sample_tweet_path = Path(project_root, \"resources/sample_tweet.json\")\n",
    "tweet_schema = schema_service.get_twitter_schema(spark=spark, twitter_sample_path=sample_tweet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "entity_comma = '&#44;'\n",
    "line_ending_pattern = re.compile(\"[\\r\\n]\")\n",
    "def clean_text(text:str):\n",
    "    result = text\n",
    "    if text is not None and len(text) > 0:\n",
    "        result = re.sub(line_ending_pattern, '', text)\n",
    "        result = re.sub(\",\", entity_comma, result)\n",
    "    return result\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "def get_cashtag_info(ticker: str, has_cashtag: bool, ticker_in_text: bool) -> Dict:\n",
    "    return {\"ticker\": ticker, \"has_cashtag\": has_cashtag, \"ticker_in_text\": ticker_in_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 856775\n"
     ]
    }
   ],
   "source": [
    "df_init = spark.read.json(str(file_path) + \"/*.txt\")\n",
    "\n",
    "# df_init = df_init.limit(100).repartition(25)\n",
    "print(f'Number of rows: {df_init.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of search tuples: 8601\n"
     ]
    }
   ],
   "source": [
    "search_tuples = twitter_service.get_ticker_searchable_tuples()\n",
    "\n",
    "print(f'number of search tuples: {len(search_tuples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798644\n"
     ]
    }
   ],
   "source": [
    "df_unduped = df_init.dropDuplicates(['id'])\n",
    "print(df_unduped.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# F.col('place.full_name').alias('place_full_name').cast(StringType()),\n",
    "\n",
    "sel_columns = ['created_at',\n",
    "'id',\n",
    "'text',\n",
    "'truncated',\n",
    "'source',\n",
    "'in_reply_to_status_id',\n",
    "'in_reply_to_user_id',\n",
    "'in_reply_to_screen_name',\n",
    "'contributors',\n",
    "'is_quote_status',\n",
    "'retweet_count',\n",
    "'favorite_count',\n",
    "'retweeted',\n",
    "'possibly_sensitive',\n",
    "'lang',\n",
    "F.col('entities.user_mentions')[0].alias('entities_user_mentions_0').cast(StringType()),\n",
    "F.col('entities.user_mentions')[1].alias('entities_user_mentions_1').cast(StringType()),\n",
    "F.col('entities.user_mentions')[2].alias('entities_user_mentions_2').cast(StringType()),\n",
    "F.col('entities.user_mentions')[3].alias('entities_user_mentions_3').cast(StringType()),\n",
    "F.col('entities.urls')[0].alias('entities_urls_0').cast(StringType()),\n",
    "F.col('entities.urls')[1].alias('entities_urls_1').cast(StringType()),\n",
    "F.col('entities.urls')[2].alias('entities_urls_2').cast(StringType()),\n",
    "F.col('entities.urls')[3].alias('entities_urls_3').cast(StringType()),\n",
    "F.col('metadata.iso_language_code').alias('metadata_iso_language_code'),\n",
    "F.col('metadata.result_type').alias('metadata_result_type'),\n",
    "F.col('user.id').alias('user_id'),\n",
    "F.col('user.name').alias('user_name'),\n",
    "F.col('user.screen_name').alias('user_screen_name'),\n",
    "F.col('user.location').alias('user_location'),\n",
    "F.col('user.description').alias('user_description'),\n",
    "F.col('user.url').alias('user_url'),\n",
    "F.col('user.protected').alias('user_protected'),\n",
    "F.col('user.followers_count').alias('user_followers_count').cast(IntegerType()),\n",
    "F.col('user.friends_count').alias('user_friends_count').cast(IntegerType()),\n",
    "F.col('user.listed_count').alias('user_listed_count'),\n",
    "F.col('user.created_at').alias('user_created_at'),\n",
    "F.col('user.favourites_count').alias('user_favourites_count').cast(IntegerType()),\n",
    "F.col('user.utc_offset').alias('user_utc_offset'),\n",
    "F.col('user.time_zone').alias('user_time_zone'),\n",
    "F.col('user.geo_enabled').alias('user_geo_enabled'),\n",
    "F.col('user.verified').alias('user_verified'),\n",
    "F.col('user.statuses_count').alias('user_statuses_count').cast(IntegerType()),\n",
    "F.col('user.lang').alias('user_lang'),\n",
    "F.col('user.contributors_enabled').alias('user_contributors_enabled'),\n",
    "F.col('user.is_translator').alias('user_is_translator'),\n",
    "F.col('user.is_translation_enabled').alias('user_is_translation_enabled'),\n",
    "F.col('user.profile_background_color').alias('user_profile_background_color'),\n",
    "F.col('user.profile_background_image_url').alias('user_profile_background_image_url'),\n",
    "F.col('user.profile_background_image_url_https').alias('user_profile_background_image_url_https'),\n",
    "F.col('user.profile_background_tile').alias('user_profile_background_tile'),\n",
    "F.col('user.profile_image_url').alias('user_profile_image_url'),\n",
    "F.col('user.profile_image_url_https').alias('user_profile_image_url_https'),\n",
    "F.col('user.profile_banner_url').alias('user_profile_banner_url'),\n",
    "F.col('user.profile_link_color').alias('user_profile_link_color'),\n",
    "F.col('user.profile_sidebar_border_color').alias('user_profile_sidebar_border_color'),\n",
    "F.col('user.profile_sidebar_fill_color').alias('user_profile_sidebar_fill_color'),\n",
    "F.col('user.profile_text_color').alias('user_profile_text_color'),\n",
    "F.col('user.profile_use_background_image').alias('user_profile_use_background_image'),\n",
    "F.col('user.has_extended_profile').alias('user_has_extended_profile'),\n",
    "F.col('user.default_profile').alias('user_default_profile'),\n",
    "F.col('user.default_profile_image').alias('user_default_profile_image'),\n",
    "F.col('user.following').alias('user_following'),\n",
    "F.col('user.follow_request_sent').alias('user_follow_request_sent'),\n",
    "F.col('user.notifications').alias('user_notifications'),\n",
    "F.col('user.translator_type').alias('user_translator_type'),\n",
    "F.col('place.country').alias('place_country').cast(StringType()),\n",
    "F.col('place.name').alias('place_name').cast(StringType())\n",
    "]\n",
    "\n",
    "df_flat = df_unduped.select(*sel_columns)\n",
    "df_thin = df_flat.drop(*['user', 'metadata', 'entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Writing Junk Journal With Coffee Stain Pages  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @ESPNNFL: Clyde Edwards-Helaire in Week 6: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sef's Kebab &amp;amp; Burger rated 1/5 MAJOR IMPRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Me: pays off all bills and feels like the quee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @RichmanPoorman8: Check out Vintage Purple ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @ralphbakshi: Gandalf at Helm's Deep. Origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @BrennenWohlford: Clyde Edwards-Helaire aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Vintage Beautiful Large Genuine Turquoise Gems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @voudaux: Rae Dunn BUNNY KISSES Ceramic Cof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @Oldglorycries: Mask it or casket?We're jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@sianthewlis I am very sorry to hear of your e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@WilliamGeer6 @eBay @AskeBay @eBayNewsroom eba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @atticesoterica: Vintage Anchor Hocking Jan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RT @AynRandPaulRyan: Omfg Trump is going to lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RT @ESPNNFL: Clyde Edwards-Helaire in Week 6: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>#Sticker for sale #Etsy for your #Crafting wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2 TUBS for the BEST TIGHT END IN THE @NFL - @t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   Writing Junk Journal With Coffee Stain Pages  ...\n",
       "1   RT @ESPNNFL: Clyde Edwards-Helaire in Week 6: ...\n",
       "2   Sef's Kebab &amp; Burger rated 1/5 MAJOR IMPRO...\n",
       "3   Me: pays off all bills and feels like the quee...\n",
       "4   RT @RichmanPoorman8: Check out Vintage Purple ...\n",
       "5   RT @ralphbakshi: Gandalf at Helm's Deep. Origi...\n",
       "6   RT @BrennenWohlford: Clyde Edwards-Helaire aft...\n",
       "7   Vintage Beautiful Large Genuine Turquoise Gems...\n",
       "8   RT @voudaux: Rae Dunn BUNNY KISSES Ceramic Cof...\n",
       "9   RT @Oldglorycries: Mask it or casket?We're jus...\n",
       "10  @sianthewlis I am very sorry to hear of your e...\n",
       "11  @WilliamGeer6 @eBay @AskeBay @eBayNewsroom eba...\n",
       "12  RT @atticesoterica: Vintage Anchor Hocking Jan...\n",
       "13  RT @AynRandPaulRyan: Omfg Trump is going to lo...\n",
       "14  RT @ESPNNFL: Clyde Edwards-Helaire in Week 6: ...\n",
       "15  #Sticker for sale #Etsy for your #Crafting wit...\n",
       "16  2 TUBS for the BEST TIGHT END IN THE @NFL - @t..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_thin.withColumn(\"text\", clean_text_udf(F.col(\"text\")))\\\n",
    "    .withColumn(\"user_name\", clean_text_udf(F.col(\"user_name\")))\\\n",
    "    .withColumn(\"user_screen_name\", clean_text_udf(F.col(\"user_screen_name\")))\\\n",
    "    .withColumn(\"user_location\", clean_text_udf(F.col(\"user_location\")))\\\n",
    "    .withColumn(\"user_description\", clean_text_udf(F.col(\"user_description\")))\\\n",
    "    .withColumn(\"entities_user_mentions_0\", clean_text_udf(F.col(\"entities_user_mentions_0\")))\\\n",
    "    .withColumn(\"entities_user_mentions_1\", clean_text_udf(F.col(\"entities_user_mentions_1\")))\\\n",
    "    .withColumn(\"entities_user_mentions_2\", clean_text_udf(F.col(\"entities_user_mentions_2\")))\\\n",
    "    .withColumn(\"entities_user_mentions_3\", clean_text_udf(F.col(\"entities_user_mentions_3\")))\\\n",
    "    .withColumn(\"entities_urls_0\", clean_text_udf(F.col(\"entities_urls_0\")))\\\n",
    "    .withColumn(\"entities_urls_1\", clean_text_udf(F.col(\"entities_urls_1\")))\\\n",
    "    .withColumn(\"entities_urls_2\", clean_text_udf(F.col(\"entities_urls_2\")))\\\n",
    "    .withColumn(\"entities_urls_3\", clean_text_udf(F.col(\"entities_urls_3\")))\\\n",
    "    .withColumn(\"place_name\", clean_text_udf(F.col(\"place_name\")))\\\n",
    "    .withColumn(\"place_country\", clean_text_udf(F.col(\"place_country\")))\\\n",
    "    .withColumn(\"user_url\", clean_text_udf(F.col(\"user_url\")))\\\n",
    "    .withColumn(\"user_profile_background_image_url\", clean_text_udf(F.col(\"user_profile_background_image_url\")))\\\n",
    "    .withColumn(\"source\", clean_text_udf(F.col(\"source\")))\\\n",
    "    .withColumn(\"in_reply_to_screen_name\", clean_text_udf(F.col(\"in_reply_to_screen_name\")))\\\n",
    "    .dropDuplicates(['id'])\n",
    "\n",
    "df_clean.select(*['text']).limit(17).toPandas().head(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "search_tuples = twitter_service.get_ticker_searchable_tuples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['created_at', 'id', 'text', 'truncated', 'source', 'in_reply_to_status_id', 'in_reply_to_user_id', 'in_reply_to_screen_name', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'retweeted', 'possibly_sensitive', 'lang', 'entities_user_mentions_0', 'entities_user_mentions_1', 'entities_user_mentions_2', 'entities_user_mentions_3', 'entities_urls_0', 'entities_urls_1', 'entities_urls_2', 'entities_urls_3', 'metadata_iso_language_code', 'metadata_result_type', 'user_id', 'user_name', 'user_screen_name', 'user_location', 'user_description', 'user_url', 'user_protected', 'user_followers_count', 'user_friends_count', 'user_listed_count', 'user_created_at', 'user_favourites_count', 'user_utc_offset', 'user_time_zone', 'user_geo_enabled', 'user_verified', 'user_statuses_count', 'user_lang', 'user_contributors_enabled', 'user_is_translator', 'user_is_translation_enabled', 'user_profile_background_color', 'user_profile_background_image_url', 'user_profile_background_image_url_https', 'user_profile_background_tile', 'user_profile_image_url', 'user_profile_image_url_https', 'user_profile_banner_url', 'user_profile_link_color', 'user_profile_sidebar_border_color', 'user_profile_sidebar_fill_color', 'user_profile_text_color', 'user_profile_use_background_image', 'user_has_extended_profile', 'user_default_profile', 'user_default_profile_image', 'user_following', 'user_follow_request_sent', 'user_notifications', 'user_translator_type', 'place_country', 'place_name', 'text_lc', 'source_lc', 'entities_user_mentions_0_lc', 'entities_user_mentions_1_lc', 'entities_user_mentions_2_lc', 'entities_user_mentions_3_lc', 'entities_urls_0_lc', 'entities_urls_1_lc', 'entities_urls_2_lc', 'entities_urls_3_lc', 'user_description_lc', 'user_url_lc']\n"
     ]
    }
   ],
   "source": [
    "columns_to_search = ['text', 'source', 'entities_user_mentions_0', 'entities_user_mentions_1', 'entities_user_mentions_2', 'entities_user_mentions_3', 'entities_urls_0', 'entities_urls_1', 'entities_urls_2', 'entities_urls_3', 'user_description', 'user_url']\n",
    "\n",
    "lc_cols = []\n",
    "for c in columns_to_search:\n",
    "    lc_cols.append(f'{c}_lc')\n",
    "    df_clean = df_clean.withColumn(f'{c}_lc', F.lower(F.col(c)))\n",
    "\n",
    "\n",
    "print(df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 930308\n",
      "Elapsed: 730.1650590156497 per second.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def get_cashtags_row_wise(row: Row):\n",
    "    cashtags_stock = []\n",
    "    \n",
    "    row_dict = row.asDict()\n",
    "    all_thing = ''\n",
    "    \n",
    "    text = ''\n",
    "    for k in row_dict.keys():\n",
    "        if k.endswith('_lc'):\n",
    "            if k == 'text_lc':\n",
    "                text = row_dict[k]\n",
    "                if text is None:\n",
    "                    text = ''\n",
    "                text_len = len(str(text))\n",
    "            else:\n",
    "                cell = row_dict[k]\n",
    "                cell = '' if cell is None else cell\n",
    "                \n",
    "                if type(cell) != 'str':\n",
    "                    cell = str(cell)\n",
    "                    \n",
    "                if cell is None:\n",
    "                    cell = ''\n",
    "                all_thing += cell \n",
    "    all_thing = text + all_thing\n",
    "            \n",
    "    for s in search_tuples:\n",
    "        ticker = s[0]\n",
    "        ticker_lc = ticker.lower()\n",
    "        name_lc = s[1].lower()\n",
    "        \n",
    "        index = all_thing.find(f'${ticker_lc}')\n",
    "        if index > -1:\n",
    "            ticker_in_text = True if index < text_len else False\n",
    "            cashtags_stock.append(get_cashtag_info(ticker=ticker, has_cashtag=True, ticker_in_text=ticker_in_text))\n",
    "        else:\n",
    "            index_ticker = all_thing.find(ticker_lc)\n",
    "            index_name = all_thing.find(name_lc)\n",
    "            \n",
    "            if index_ticker > -1 and index_name > -1:\n",
    "                ticker_in_text = True if index_ticker < text_len else False\n",
    "                cashtags_stock.append(get_cashtag_info(ticker=ticker, has_cashtag=False, ticker_in_text=ticker_in_text))\n",
    "                \n",
    "        num_other_tickers = len(cashtags_stock) - 1\n",
    "        for tag in cashtags_stock:\n",
    "            tag['num_other_tickers_in_tweet'] = num_other_tickers\n",
    "    \n",
    "    return cashtags_stock\n",
    "          \n",
    "schema = ArrayType(StructType(fields=[StructField('ticker', StringType()),\n",
    "                                      StructField('has_cashtag', BooleanType()),\n",
    "                                      StructField('ticker_in_text', BooleanType()),\n",
    "                                      StructField('num_other_tickers_in_tweet', IntegerType())\n",
    "                                     ]))\n",
    "get_cashtags_row_wise_udf = udf(get_cashtags_row_wise, schema)\n",
    "\n",
    "# df_tmp = df_clean.limit(10)\n",
    "\n",
    "df_f22_flagged = df_clean.withColumn(\"f22\", get_cashtags_row_wise_udf((struct([df_clean[x] for x in df_clean.columns]))))\n",
    "\n",
    "df_f22_exploded = df_f22_flagged.withColumn('f22', explode(F.col('f22')))\n",
    "\n",
    "se_columns = list(set(df_f22_exploded.columns) - set(lc_cols)) + [F.col('f22.ticker').alias('f22_ticker'),\n",
    "                                        F.col('f22.has_cashtag').alias('f22_has_cashtag'),\n",
    "                                        F.col('f22.ticker_in_text').alias('f22_ticker_in_text'),\n",
    "                                        F.col('f22.num_other_tickers_in_tweet').alias('f22_num_other_tickers_in_tweet')\n",
    "                                       ]\n",
    "\n",
    "df_tickered = df_f22_exploded.select(*se_columns).drop('f22')\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "total = df_tickered.count()\n",
    "print(f'Count: {total}')\n",
    "end = time.time()\n",
    "\n",
    "sec_per_record = total / (end - start)\n",
    "\n",
    "print(f'Elapsed: {sec_per_record} per second.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\tweets_flat_2020-10-25_16-36-14-314.4\n"
     ]
    }
   ],
   "source": [
    "from ams.services import dataframe_services\n",
    "\n",
    "flat_drop_path = Path(data_path, twitter_folder, 'flattened_drop')\n",
    "prefix = \"tweets_flat\"\n",
    "\n",
    "dataframe_services.persist_dataframe_as_csv(df=df_tickered, output_drop_folder_path=flat_drop_path, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
