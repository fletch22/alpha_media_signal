{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Chris\\\\workspaces\\\\alpha_media_signal\\\\ams\\\\notebooks\\\\twitter', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\alpha_media_signal', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\open_source\\\\spark-3.0.1-bin-hadoop2.7\\\\python', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\open_source\\\\spark-3.0.1-bin-hadoop2.7\\\\python\\\\lib\\\\py4j-0.10.9-src.zip', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal\\\\python37.zip', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal\\\\DLLs', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal\\\\lib', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal', '', 'C:\\\\Users\\\\Chris\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages\\\\win32', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\programdata\\\\miniconda3\\\\envs\\\\alpha_media_signal\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Chris\\\\.ipython', '/home/jovyan/work']\n",
      "Setting up logging...\n",
      "Will use logging path: C:\\Users\\Chris\\workspaces\\data\\logs\\alpha_media_signal\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "paths_to_add = ['/home/jovyan/work']\n",
    "\n",
    "for p in paths_to_add:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "\n",
    "print(sys.path)\n",
    "native_spark = True\n",
    "\n",
    "import pandas as pd\n",
    "from ams.services import spark_service\n",
    "\n",
    "import findspark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, udf\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from ams.services import twitter_service, file_services\n",
    "from pyspark.sql import functions as F\n",
    "from pathlib import Path\n",
    "from pyspark.sql.types import StringType, StructType, StructField, BooleanType, MapType, ArrayType, Row\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import re\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.types import IntegerType\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "spark = spark_service.get_or_create(app_name='twitter_flatten')\n",
    "sc = spark.sparkContext\n",
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(__name__)\n",
    "LOGGER.info(\"pyspark script logger initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ams.config import constants\n",
    "\n",
    "if native_spark:\n",
    "    project_root = \"../../../\"\n",
    "    data_path = Path(constants.DATA_PATH)\n",
    "else:\n",
    "    data_path = Path('/home/jovyan/work/data/')\n",
    "    project_root = \"/home/jovyan/work/\"\n",
    "    \n",
    "twitter_folder = 'twitter'\n",
    "\n",
    "file_path = Path(data_path, twitter_folder, 'fixed_drop', 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ams.services import schema_service\n",
    "\n",
    "sample_tweet_path = Path(project_root, \"resources/sample_tweet.json\")\n",
    "tweet_schema = schema_service.get_twitter_schema(spark=spark, twitter_sample_path=sample_tweet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "entity_comma = '&#44;'\n",
    "line_ending_pattern = re.compile(\"[\\r\\n]\")\n",
    "def clean_text(text:str):\n",
    "    result = text\n",
    "    if text is not None and len(text) > 0:\n",
    "        result = re.sub(line_ending_pattern, '', text)\n",
    "        result = re.sub(\",\", entity_comma, result)\n",
    "    return result\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "def get_cashtag_info(ticker: str, has_cashtag: bool, ticker_in_text: bool) -> Dict:\n",
    "    return {\"ticker\": ticker, \"has_cashtag\": has_cashtag, \"ticker_in_text\": ticker_in_text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of search tuples: 10462\n"
     ]
    }
   ],
   "source": [
    "search_tuples = twitter_service.get_ticker_searchable_tuples()\n",
    "\n",
    "print(f'number of search tuples: {len(search_tuples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "def fix_columns(df: DataFrame):\n",
    "    \n",
    "    sel_columns = ['created_at',\n",
    "    'id',\n",
    "    'text',\n",
    "    'truncated',\n",
    "    'source',\n",
    "    'in_reply_to_status_id',\n",
    "    'in_reply_to_user_id',\n",
    "    'in_reply_to_screen_name',\n",
    "    'contributors',\n",
    "    'is_quote_status',\n",
    "    'retweet_count',\n",
    "    'favorite_count',\n",
    "    'retweeted',\n",
    "    'possibly_sensitive',\n",
    "    'lang',\n",
    "    F.col('entities.user_mentions')[0].alias('entities_user_mentions_0').cast(StringType()),\n",
    "    F.col('entities.user_mentions')[1].alias('entities_user_mentions_1').cast(StringType()),\n",
    "    F.col('entities.user_mentions')[2].alias('entities_user_mentions_2').cast(StringType()),\n",
    "    F.col('entities.user_mentions')[3].alias('entities_user_mentions_3').cast(StringType()),\n",
    "    F.col('entities.urls')[0].alias('entities_urls_0').cast(StringType()),\n",
    "    F.col('entities.urls')[1].alias('entities_urls_1').cast(StringType()),\n",
    "    F.col('entities.urls')[2].alias('entities_urls_2').cast(StringType()),\n",
    "    F.col('entities.urls')[3].alias('entities_urls_3').cast(StringType()),\n",
    "    F.col('metadata.iso_language_code').alias('metadata_iso_language_code'),\n",
    "    F.col('metadata.result_type').alias('metadata_result_type'),\n",
    "    F.col('user.id').alias('user_id'),\n",
    "    F.col('user.name').alias('user_name'),\n",
    "    F.col('user.screen_name').alias('user_screen_name'),\n",
    "    F.col('user.location').alias('user_location'),\n",
    "    F.col('user.description').alias('user_description'),\n",
    "    F.col('user.url').alias('user_url'),\n",
    "    F.col('user.protected').alias('user_protected'),\n",
    "    F.col('user.followers_count').alias('user_followers_count').cast(IntegerType()),\n",
    "    F.col('user.friends_count').alias('user_friends_count').cast(IntegerType()),\n",
    "    F.col('user.listed_count').alias('user_listed_count'),\n",
    "    F.col('user.created_at').alias('user_created_at'),\n",
    "    F.col('user.favourites_count').alias('user_favourites_count').cast(IntegerType()),\n",
    "    F.col('user.utc_offset').alias('user_utc_offset'),\n",
    "    F.col('user.time_zone').alias('user_time_zone'),\n",
    "    F.col('user.geo_enabled').alias('user_geo_enabled'),\n",
    "    F.col('user.verified').alias('user_verified'),\n",
    "    F.col('user.statuses_count').alias('user_statuses_count').cast(IntegerType()),\n",
    "    F.col('user.lang').alias('user_lang'),\n",
    "    F.col('user.contributors_enabled').alias('user_contributors_enabled'),\n",
    "    F.col('user.is_translator').alias('user_is_translator'),\n",
    "    F.col('user.is_translation_enabled').alias('user_is_translation_enabled'),\n",
    "    F.col('user.profile_background_color').alias('user_profile_background_color'),\n",
    "    F.col('user.profile_background_image_url').alias('user_profile_background_image_url'),\n",
    "    F.col('user.profile_background_image_url_https').alias('user_profile_background_image_url_https'),\n",
    "    F.col('user.profile_background_tile').alias('user_profile_background_tile'),\n",
    "    F.col('user.profile_image_url').alias('user_profile_image_url'),\n",
    "    F.col('user.profile_image_url_https').alias('user_profile_image_url_https'),\n",
    "    F.col('user.profile_banner_url').alias('user_profile_banner_url'),\n",
    "    F.col('user.profile_link_color').alias('user_profile_link_color'),\n",
    "    F.col('user.profile_sidebar_border_color').alias('user_profile_sidebar_border_color'),\n",
    "    F.col('user.profile_sidebar_fill_color').alias('user_profile_sidebar_fill_color'),\n",
    "    F.col('user.profile_text_color').alias('user_profile_text_color'),\n",
    "    F.col('user.profile_use_background_image').alias('user_profile_use_background_image'),\n",
    "    F.col('user.has_extended_profile').alias('user_has_extended_profile'),\n",
    "    F.col('user.default_profile').alias('user_default_profile'),\n",
    "    F.col('user.default_profile_image').alias('user_default_profile_image'),\n",
    "    F.col('user.following').alias('user_following'),\n",
    "    F.col('user.follow_request_sent').alias('user_follow_request_sent'),\n",
    "    F.col('user.notifications').alias('user_notifications'),\n",
    "    F.col('user.translator_type').alias('user_translator_type'),\n",
    "    F.col('f22_place.place_country').alias('place_country').cast(StringType()),\n",
    "    F.col('f22_place.place_full_name').alias('place_full_name').cast(StringType()),\n",
    "    F.col('f22_place.place_name').alias('place_name').cast(StringType())\n",
    "    ]\n",
    "\n",
    "    df = df.select(*sel_columns)\n",
    "    return df.drop(*['user', 'metadata', 'entities', 'f22_place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_columns(df: DataFrame):\n",
    "    return df.withColumn(\"text\", clean_text_udf(F.col(\"text\")))\\\n",
    "        .withColumn(\"user_name\", clean_text_udf(F.col(\"user_name\")))\\\n",
    "        .withColumn(\"user_screen_name\", clean_text_udf(F.col(\"user_screen_name\")))\\\n",
    "        .withColumn(\"user_location\", clean_text_udf(F.col(\"user_location\")))\\\n",
    "        .withColumn(\"user_description\", clean_text_udf(F.col(\"user_description\")))\\\n",
    "        .withColumn(\"entities_user_mentions_0\", clean_text_udf(F.col(\"entities_user_mentions_0\")))\\\n",
    "        .withColumn(\"entities_user_mentions_1\", clean_text_udf(F.col(\"entities_user_mentions_1\")))\\\n",
    "        .withColumn(\"entities_user_mentions_2\", clean_text_udf(F.col(\"entities_user_mentions_2\")))\\\n",
    "        .withColumn(\"entities_user_mentions_3\", clean_text_udf(F.col(\"entities_user_mentions_3\")))\\\n",
    "        .withColumn(\"entities_urls_0\", clean_text_udf(F.col(\"entities_urls_0\")))\\\n",
    "        .withColumn(\"entities_urls_1\", clean_text_udf(F.col(\"entities_urls_1\")))\\\n",
    "        .withColumn(\"entities_urls_2\", clean_text_udf(F.col(\"entities_urls_2\")))\\\n",
    "        .withColumn(\"entities_urls_3\", clean_text_udf(F.col(\"entities_urls_3\")))\\\n",
    "        .withColumn(\"place_name\", clean_text_udf(F.col(\"place_name\")))\\\n",
    "        .withColumn(\"user_url\", clean_text_udf(F.col(\"user_url\")))\\\n",
    "        .withColumn(\"user_profile_background_image_url\", clean_text_udf(F.col(\"user_profile_background_image_url\")))\\\n",
    "        .withColumn(\"source\", clean_text_udf(F.col(\"source\")))\\\n",
    "        .withColumn(\"in_reply_to_screen_name\", clean_text_udf(F.col(\"in_reply_to_screen_name\")))\\\n",
    "        .withColumn(\"place_country\", clean_text_udf(F.col(\"place_country\")))\\\n",
    "        .dropDuplicates(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "search_tuples = twitter_service.get_ticker_searchable_tuples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cashtags_row_wise(row: Row):\n",
    "    cashtags_stock = []\n",
    "    \n",
    "    row_dict = row.asDict()\n",
    "    all_thing = ''\n",
    "    \n",
    "    text = ''\n",
    "    for k in row_dict.keys():\n",
    "        if k.endswith('_lc'):\n",
    "            if k == 'text_lc':\n",
    "                text = row_dict[k]\n",
    "                if text is None:\n",
    "                    text = ''\n",
    "                text_len = len(str(text))\n",
    "            else:\n",
    "                cell = row_dict[k]\n",
    "                cell = '' if cell is None else cell\n",
    "                \n",
    "                if type(cell) != 'str':\n",
    "                    cell = str(cell)\n",
    "                    \n",
    "                if cell is None:\n",
    "                    cell = ''\n",
    "                all_thing += cell \n",
    "    all_thing = text + all_thing\n",
    "            \n",
    "    for s in search_tuples:\n",
    "        ticker = s[0]\n",
    "        ticker_lc = ticker.lower()\n",
    "        name_lc = s[1].lower()\n",
    "        \n",
    "        index = all_thing.find(f'${ticker_lc}')\n",
    "        if index > -1:\n",
    "            ticker_in_text = True if index < text_len else False\n",
    "            cashtags_stock.append(get_cashtag_info(ticker=ticker, has_cashtag=True, ticker_in_text=ticker_in_text))\n",
    "        else:\n",
    "            index_ticker = all_thing.find(ticker_lc)\n",
    "            index_name = all_thing.find(name_lc)\n",
    "            \n",
    "            if index_ticker > -1 and index_name > -1:\n",
    "                ticker_in_text = True if index_ticker < text_len else False\n",
    "                cashtags_stock.append(get_cashtag_info(ticker=ticker, has_cashtag=False, ticker_in_text=ticker_in_text))\n",
    "                \n",
    "        num_other_tickers = len(cashtags_stock) - 1\n",
    "        for tag in cashtags_stock:\n",
    "            tag['num_other_tickers_in_tweet'] = num_other_tickers\n",
    "    \n",
    "    return cashtags_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_tickers_and_explode(df: DataFrame):\n",
    "    \n",
    "    columns_to_search = ['text', 'source', 'entities_user_mentions_0', 'entities_user_mentions_1', 'entities_user_mentions_2', 'entities_user_mentions_3', 'entities_urls_0', 'entities_urls_1', 'entities_urls_2', 'entities_urls_3', 'user_description', 'user_url']\n",
    "    \n",
    "    lc_cols = []\n",
    "    for c in columns_to_search:\n",
    "        lc_cols.append(f'{c}_lc')\n",
    "        df = df.withColumn(f'{c}_lc', F.lower(F.col(c)))\n",
    "\n",
    "          \n",
    "    schema = ArrayType(StructType(fields=[StructField('ticker', StringType()),\n",
    "                                          StructField('has_cashtag', BooleanType()),\n",
    "                                          StructField('ticker_in_text', BooleanType()),\n",
    "                                          StructField('num_other_tickers_in_tweet', IntegerType())\n",
    "                                         ]))\n",
    "    get_cashtags_row_wise_udf = udf(get_cashtags_row_wise, schema)\n",
    "\n",
    "    df = df.withColumn(\"f22\", get_cashtags_row_wise_udf((struct([df[x] for x in df.columns]))))\n",
    "\n",
    "    df = df.withColumn('f22', explode(F.col('f22')))\n",
    "\n",
    "    se_columns = list(set(df.columns) - set(lc_cols)) + [F.col('f22.ticker').alias('f22_ticker'),\n",
    "                                            F.col('f22.has_cashtag').alias('f22_has_cashtag'),\n",
    "                                            F.col('f22.ticker_in_text').alias('f22_ticker_in_text'),\n",
    "                                            F.col('f22.num_other_tickers_in_tweet').alias('f22_num_other_tickers_in_tweet')\n",
    "                                           ]\n",
    "\n",
    "    return df.select(*se_columns).drop('f22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ams.services import dataframe_services \n",
    "from ams.services.dataframe_services import PersistedDataFrameTypes\n",
    "from retry import retry\n",
    "\n",
    "flat_drop_path = Path(data_path, twitter_folder, 'flattened_drop', \"main\")\n",
    "prefix = \"tweets_flat\"\n",
    "\n",
    "file_type = PersistedDataFrameTypes.PARQUET\n",
    "\n",
    "@retry(tries=3)\n",
    "def persist(df: DataFrame):\n",
    "    dataframe_services.persist_dataframe(df=df, \n",
    "                                         output_drop_folder_path=flat_drop_path, \n",
    "                                         prefix=prefix, \n",
    "                                         num_output_files=20,\n",
    "                                         file_type=file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unexpected_null_column(place:str):\n",
    "    result = {\"place_country\": None, \"place_full_name\": None, \"place_name\": None}\n",
    "    if place is not None:\n",
    "        result[\"place_country\"] = place[\"country\"]\n",
    "        result[\"place_full_name\"] = place[\"full_name\"]\n",
    "        result[\"place_name\"] = place[\"name\"]\n",
    "    return result\n",
    "\n",
    "schema = StructType(fields=[StructField('place_country', StringType()),\n",
    "                                          StructField('place_full_name', StringType()),\n",
    "                                          StructField('place_name', StringType()),\n",
    "                                         ])\n",
    "\n",
    "clean_unexpected_null_column_udf = udf(clean_unexpected_null_column, schema)\n",
    "\n",
    "def clean_place(df: DataFrame):\n",
    "    \n",
    "    df = df.withColumn(\"f22_place\", clean_unexpected_null_column_udf(F.col(\"place\")))\n",
    "    df = df.drop(\"place\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-53-46-536.73.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-54-04-281.73.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-54-27-341.73.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_09-58-17-345.94\n",
      "Processing 2 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-54-47-517.73.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-55-03-273.73.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-55-18-754.73.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-02-51-13.33\n",
      "Processing 3 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-55-33-555.73.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-55-41-190.73.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-55-51-572.58.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-07-27-730.3\n",
      "Processing 4 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-56-01-168.58.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-56-11-819.97.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-56-22-47.97.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-11-58-109.86\n",
      "Processing 5 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-56-33-619.0.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-56-43-622.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-56-46-107.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-16-33-407.45\n",
      "Processing 6 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-56-51-644.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-56-53-102.0.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-57-03-53.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-21-06-457.36\n",
      "Processing 7 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-57-13-970.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-57-25-217.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-57-36-994.0.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-25-22-394.34\n",
      "Processing 8 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-57-47-628.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-57-56-908.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-58-07-939.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-29-56-857.95\n",
      "Processing 9 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-58-18-264.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-58-28-30.0.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-58-38-945.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-34-31-615.09\n",
      "Processing 10 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-58-49-668.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-59-01-522.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-59-09-322.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-39-07-994.78\n",
      "Processing 11 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-59-19-717.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-59-30-28.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-59-41-637.0.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-43-43-715.78\n",
      "Processing 12 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_22-59-52-94.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-00-04-211.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-00-16-79.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-48-19-207.12\n",
      "Processing 13 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-00-27-635.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-00-31-496.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-00-41-487.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-52-54-878.99\n",
      "Processing 14 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-00-51-346.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-01-02-336.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-01-12-5.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_10-57-21-668.7\n",
      "Processing 15 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-01-23-416.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-01-33-308.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-01-43-151.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-01-56-945.2\n",
      "Processing 16 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-01-48-740.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-01-58-221.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-02-07-819.01.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-06-33-384.04\n",
      "Processing 17 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-02-18-562.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-02-29-761.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-02-40-867.99.txt_fixed.txt']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-11-11-447.7\n",
      "Processing 18 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-02-51-886.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-03-02-689.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-03-06-204.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-15-45-714.19\n",
      "Processing 19 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-03-16-206.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-03-25-444.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-03-37-396.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-20-20-807.48\n",
      "Processing 20 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-03-47-718.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-03-59-958.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-10-362.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-24-58-794.15\n",
      "Processing 21 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-15-230.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-16-150.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-17-601.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-29-32-721.93\n",
      "Processing 22 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-21-135.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-21-316.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-21-531.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-33-54-776.89\n",
      "Processing 23 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-21-61.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-22-722.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-24-557.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-38-09-317.87\n",
      "Processing 24 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-24-916.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-26-661.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-34-965.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-42-22-804.01\n",
      "Processing 25 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-45-175.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-04-58-680.99.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-05-02-560.99.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-46-41-829.22\n",
      "Processing 26 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-05-14-979.28.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-05-36-263.28.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-05-58-177.28.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-51-31-188.86\n",
      "Processing 27 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-06-12-347.28.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-06-27-140.28.txt_fixed.txt', 'C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-06-41-908.28.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_11-56-24-878.56\n",
      "Processing 28 of 28.\n",
      "Processing files: ['C:\\\\Users\\\\Chris\\\\workspaces\\\\data\\\\twitter\\\\fixed_drop\\\\main\\\\smallified_2021-01-04_23-06-55-600.28.txt_fixed.txt']\n",
      "C:\\Users\\Chris\\workspaces\\data\\twitter\\flattened_drop\\main\\tweets_flat_2021-01-07_12-01-17-633.69\n",
      "Wall time: 2h 7min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from ams.services import file_services\n",
    "\n",
    "files = file_services.list_files(parent_path=file_path, ends_with=\".txt\")\n",
    "files = [str(f) for f in files]\n",
    "\n",
    "def chunk(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "chunked_list = list(chunk(files, 3))\n",
    "\n",
    "tot_chunks = len(chunked_list)\n",
    "\n",
    "for ndx, chunk in enumerate(chunked_list):\n",
    "#     if ndx + 1 < 22: \n",
    "#         continue\n",
    "        \n",
    "    print(f\"Processing {ndx + 1} of {tot_chunks}.\")\n",
    "    \n",
    "    print(f\"Processing files: {chunk}\")\n",
    "    \n",
    "    df_init = spark.read.json(chunk[0])\n",
    "\n",
    "    df_unduped = df_init.dropDuplicates(['id'])\n",
    "\n",
    "    df_clean_place = clean_place(df=df_unduped)\n",
    "        \n",
    "    df_thin = fix_columns(df=df_clean_place)\n",
    "    \n",
    "    df_clean = clean_columns(df=df_thin)\n",
    "    \n",
    "    df_tickered = find_tickers_and_explode(df=df_clean)\n",
    "        \n",
    "    persist(df=df_tickered)\n",
    "#     persist(df=df_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
