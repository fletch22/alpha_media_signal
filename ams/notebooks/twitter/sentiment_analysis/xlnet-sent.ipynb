{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.9\n",
      "IPython 7.19.0\n",
      "\n",
      "numpy 1.19.4\n",
      "pandas 1.1.4\n",
      "torch 1.7.0\n",
      "transformers 3.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "paths_to_add = ['/home/jovyan/work', '/home/jupyter/alpha_media_signal']\n",
    "\n",
    "for p in paths_to_add:\n",
    "    if p not in sys.path:\n",
    "        sys.path.append(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from ams.services import file_services\n",
    "from ams.services.twitter_service import balance_df\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext watermark\n",
    "\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 970'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ams.config import constants\n",
    "\n",
    "file_path = constants.TWITTER_TEXT_LABEL_TRAIN_PATH\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "df = df.sample(frac=.1)\n",
    "\n",
    "df[\"npl_text\"] = df[\"npl_text\"].astype('str')\n",
    "\n",
    "num_rows = df.shape[0]\n",
    "train_num = math.ceil(num_rows * .98)\n",
    "\n",
    "df = df.sort_values(by=[\"date\"], ascending=True)\n",
    "\n",
    "df_train = df.iloc[:train_num]\n",
    "df_train = balance_df(df_train)\n",
    "df_train = df_train.sort_values(by=[\"date\"], ascending=True)\n",
    "df_test = df.iloc[train_num:]\n",
    "df_test = balance_df(df_test)\n",
    "df_test = df_test.sort_values(by=[\"date\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f22_id</th>\n",
       "      <th>text</th>\n",
       "      <th>buy_sell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1340852</th>\n",
       "      <td>7061548141802896666</td>\n",
       "      <td>RT @jendeukiebabo: BLACKPINK SCHOOL Class Elec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210198</th>\n",
       "      <td>-6676452524606462930</td>\n",
       "      <td>Check out Townhead Alston. Posted 1910 https:/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815778</th>\n",
       "      <td>694424777754236834</td>\n",
       "      <td>15 bucks -------- RT if good deal | like if li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726079</th>\n",
       "      <td>-395339481003766629</td>\n",
       "      <td>RT @GPHlIC: 2000s apple iMac g3 / iBook https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496285</th>\n",
       "      <td>8951472720249049856</td>\n",
       "      <td>RT @HARUW00S: the only boy performance where w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13138</th>\n",
       "      <td>-9064524891594402422</td>\n",
       "      <td>Halloween Party Treat Bag https://t.co/uef1xM3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719496</th>\n",
       "      <td>-475171500344947582</td>\n",
       "      <td>@EliotETC We export 40min to 2 hour shows at m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53555</th>\n",
       "      <td>-8576551148855660334</td>\n",
       "      <td>Schneider Electric will integrate #Fortinet's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503122</th>\n",
       "      <td>9033225897521310249</td>\n",
       "      <td>@bigpIoopy this video is the peak all videos g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446972</th>\n",
       "      <td>-3796615969759569759</td>\n",
       "      <td>Lululemon Athletica $LULU Upgraded at BidaskCl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      f22_id  \\\n",
       "1340852  7061548141802896666   \n",
       "210198  -6676452524606462930   \n",
       "815778    694424777754236834   \n",
       "726079   -395339481003766629   \n",
       "1496285  8951472720249049856   \n",
       "13138   -9064524891594402422   \n",
       "719496   -475171500344947582   \n",
       "53555   -8576551148855660334   \n",
       "1503122  9033225897521310249   \n",
       "446972  -3796615969759569759   \n",
       "\n",
       "                                                      text  buy_sell  \n",
       "1340852  RT @jendeukiebabo: BLACKPINK SCHOOL Class Elec...         0  \n",
       "210198   Check out Townhead Alston. Posted 1910 https:/...         1  \n",
       "815778   15 bucks -------- RT if good deal | like if li...         1  \n",
       "726079   RT @GPHlIC: 2000s apple iMac g3 / iBook https:...         1  \n",
       "1496285  RT @HARUW00S: the only boy performance where w...         0  \n",
       "13138    Halloween Party Treat Bag https://t.co/uef1xM3...         0  \n",
       "719496   @EliotETC We export 40min to 2 hour shows at m...         1  \n",
       "53555    Schneider Electric will integrate #Fortinet's ...         1  \n",
       "1503122  @bigpIoopy this video is the peak all videos g...         1  \n",
       "446972   Lululemon Athletica $LULU Upgraded at BidaskCl...         1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweets = df_train.nlp_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [tweet + \" [SEP] [CLS]\" for tweet in tweets]\n",
    "labels = df_train.buy_sell.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁', 'rt', '▁', '@', 'crystal', 'moon', '1', 'ba', ':', '▁ends', '▁10', '/', '05', ':', '▁for', '▁sale', '▁', '/', '▁best', '▁offer', '▁', '-', '▁', '#', 'amazon', 'ite', '▁', '#', 'car', 'ne', 'lian', '▁', '#', 'ster', 'ling', 'silver', '▁', '#', 'wire', 'wrapped', '▁', '#', 'cha', 'kra', '▁', '#', 're', 'iki', 'he', 'al', 'ing', '▁', '#', 'he', 'al', 'ing', 'p', '.', '.', '.', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in tweets]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "MAX_LEN = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chris\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `mem_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2, mem_len=1024)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 230.00 MiB (GPU 0; 4.00 GiB total capacity; 2.63 GiB already allocated; 117.72 MiB free; 2.73 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-311c978fa2c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1510\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1511\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1512\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1513\u001b[0m         )\n\u001b[0;32m   1514\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1238\u001b[0m                 \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1241\u001b[0m             )\n\u001b[0;32m   1242\u001b[0m             \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         )\n\u001b[0;32m    527\u001b[0m         \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[0mattn_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattn_mask_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m             )\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mrel_attn_core\u001b[1;34m(self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# position based attention score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[0mbd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ibnd,jbnd->bnij\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_head\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr_r_bias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_head_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[0mbd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrel_shift_bnij\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mklen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(equation, *operands)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 230.00 MiB (GPU 0; 4.00 GiB total capacity; 2.63 GiB already allocated; 117.72 MiB free; 2.73 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    b_input_ids= b_input_ids.type(torch.LongTensor)\n",
    "    b_input_mask= b_input_mask.type(torch.LongTensor)\n",
    "    b_labels= b_labels.type(torch.LongTensor)\n",
    "    \n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    b_input_mask = b_input_mask.to(device)\n",
    "    b_labels = b_labels.to(device)\n",
    "    \n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    b_input_ids= b_input_ids.type(torch.LongTensor)\n",
    "    b_input_mask= b_input_mask.type(torch.LongTensor)\n",
    "    b_labels= b_labels.type(torch.LongTensor)\n",
    "    \n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    b_input_mask = b_input_mask.to(device)\n",
    "    b_labels = b_labels.to(device)\n",
    "    \n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      logits = output[0]\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = constants.COLA_OUT_OF_DOMAIN_DEV_PATH\n",
    "# df = pd.read_csv(file_path, delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "\n",
    "# Create sentence and label lists\n",
    "tweets = df_test.text.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n",
    "tweets = [tweet + \" [SEP] [CLS]\" for tweet in tweets]\n",
    "labels = df_test.buy_sell.values\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in tweets]\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "  \n",
    "batch_size = 32  \n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b05e9e6622af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Forward pass, calculate logit predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1510\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1511\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1512\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1513\u001b[0m         )\n\u001b[0;32m   1514\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1168\u001b[0m             \u001b[0mword_emb_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1170\u001b[1;33m             \u001b[0mword_emb_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1171\u001b[0m         \u001b[0moutput_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_emb_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget_mapping\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m         return F.embedding(\n\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\envs\\alpha_media_signal\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_set = []\n",
    "\n",
    "for i in range(len(true_labels)):\n",
    "  matthews = matthews_corrcoef(true_labels[i],\n",
    "                 np.argmax(predictions[i], axis=1).flatten())\n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_corrcoef(flat_true_labels, flat_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = flat_accuracy(predictions, true_labels)\n",
    "\n",
    "print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "model_path = file_services.create_unique_filename(parent_dir=constants.TWITTER_MODEL_PATH, prefix=\"xlnet-sent\", extension=\"mod\")\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}