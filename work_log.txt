22 April:

    columns = [c for c in df_rem.columns if str(df_rem[c].dtype) == "object"]
    columns.remove("ticker")
    df_one_hotted = make_one_hotted(df=df_rem, df_all_tickers=df_all_tickers, cols=columns)

    # df_ticker_hotted, narrow_cols = one_hot(df=df_refined)

19 April:

    Change weights:
        User power 3, for open * volume
        Try with agg tipranks instead of add_tipranks
        Revert back to weight power 3 for open only


April 10th:

    Ideas:
    * Use prediction sum instead of just in all three.


April 9th:

	Try:

		With/Without first 10k rows.
            Status: Worked Great!
		With/Without tipranks.
            Status: Better without tip ranks.
		Inner join on tipranks in sm_proc.
            Status: Bad. Not enough df_predict.

March 27th:

    Save deque of models + purchase_date
    As we progress backwards through days, pop models from deque + df_test (for that day) and persist prediction. Do
        this for all 3 models such that there is a prediction row for all 3 models.

March 26th:

    TODO: Run REAL predictions against all past days.
    TODO: Then fd out if stock up/down affected roi on purchase date.


March 25th:

    Try sample weighting again.

    Why didn't GridCVS work?

    TODO: Run stock_merge and make_preds on reprocessed.
    TODO: Run sm and mp
        - What is performance when we combine the model outputs?
        - what happens when the stock is down from day previous to purchase day?
        - what happens if we purchse the stock on open instead of at eod?

March 20th:

    Weighting the training labels so that precision is optimized will be ideal. That is, precision=1.0 means
    there are no false positives.

    TODO: Add filter step: do not use if symbol is not in upper case and is not mentioned by full name and does not use $symbol format.

    TODO: Re-download data from the 18th and the 19th and 21st, + 22nd

March 15th:

    TODO: Adjust date (or some other new date field) to reflect the previous market it applies to if it arrived > close and < midnight. Perform this step in
    stock merge for Sat and Sun tweets - create new field called "adjust_market_date_str"; add num days between end of market and tweet time.

March 12th:

    ToDo: Convert historical model to pipe. Output is dataframe.
    Todo: Convert model_excercise to pipe. Ouput is models.
    Todo: Reconsider stacking. Is there a way to test without leakage?
    ToDo: Fit piping logic to fit with Pipe class.

    Weekend Todo:

        Step 1. Fix twitter_service ability to detect missing data
                Status: Actually not broken.
        Step 2. Convert twitter_ml_utils to pipe. Must be single df. Add columns for revolution; consider splitting
                into revolutions at all at once; at train time (next pipe).
                Status:
        Step 3. Convert model_excercise to pipe (either perf testing or prediction)
        Step 4. Add weights to XGBRegressor - more recent more weight.
        Step 5. Start testing CatBoost Regression.

March 11th:

	.008 with using buy_sell_predictions + regression using "stocking_val_change" - (buy all preds)
	1.1,.009.1.3,1.2: Above top of 1st quintile

	0.007: above top of 1st quintile; min_price 5.;
	0.0046 min_price 5.;
	0.005; above top of 1st quartile; min_price 5.;
	0.0088; above top of 1st sixtile; min_price 5.;
    0.0081; above top of 1st septile; min_price 5.;


March 10th:

    ToDo: Finish modding merge_with_stock. Bolt on train+prediction by spinning through data files.
    ToDo: Test with Regression.
    ToDo: Finish stacking.
    ToDo: Pull out December data.
    ToDo: Add supplement code when new tweet data comes in.


March 9th 2021:

	MLP (100, 100, 100); 10: 0.5393258426966292

March 8th 2021:

    TODO: Productionize test_stacking.py


March 7th 2021:

	TODO: Enhance pipeline framework to define linkages to be used by all pipes. A linkage defines input and output paths - also behaviors?

March 6th 2021:
	
	Pred with Class: .06395038158995493
	Pred Reg 3 deep: 0.028 
	Pred Reg 8 deep: 0.0332039357904556, 0.03586278507792609
	Pred Reg 11 deep: .025

	TODO: Add t_bill change in last: 1 day, 2 day, 3 day
	TODO: Test with same day open->close.


March 4th 2021:

    Use stratify to use unbalanced data.
    Pass verbose to XGBoost fit
    Use early_stopping_rounds=10
    Use eval_metric 'aucpr'
    Use scale_pos_weight to help unbalanced data.
    Use XGBClassifier.fit's eval_set=[(X_test, y_test)] argument.
        This might sharpen predictions because these test affect the number of trees used while training.


Feb 22 2021:
	
	Fix tweet date attribution.
	Get list of tweets dates. divide in list of 2.
	Take the 1st of list.
	Get all market dates in range.
	Get tweet date + purchase_dt + sell_dt - add to used dates until get to recent.
	Remove sell dates from market dates

	Todo: Remove increment_day_if_tweet_after_hours when re-processing.



Feb 18th 2021:

	0.06065112554627501
	0.08109058192982933
	0.0549
	0.055048567929930514

	0.075
	0.07534162094891027
	0.0691
	0.072
	0.07569
	2021-01-13: roi: 0.20024690494658443: 174 tickers

Feb 17th 2021:

    ToDo: Add Zip_Service; add step to pipe_fitter to zip up raw files.
    ToDo: Add raw tweet unzipper.

Feb 16th, 2021:

    ToDo: Incorporate split logic in pred_perf, historical preds, etc.

	XGBoost:

		1. Predict a split line (initially use label halfway between 2 labels; e.g., .5)
		2. Split into 2 leaves by choosing a random split point in the data.
		3. This will create 2 leaves of 2 sets of data.
		4. Calc similarity score for the set in each leaf:

			residual == prediction - label value
			res = residual

			simX = (res1 + res2 + res3)^2/(num_samples + lambda)

		5. Gain is the Sum of all the similarities minus the root similarity

			Gain = sim1 + sim2 + sim3 - simm_root

		6. Start again with a new decision threshold. Compare the gain.

		7. Default level of splits == 6

		8. Pruning branches. 

			Using gamma variable (default 130), we subtract gamma from gain. If the result is negative, we remove the branch.

		9. When lambda > 0, it is easier to prune leaves because gains are smaller (due to larger divisor)

		10. For each leaf: 
				Cal the output value:
					sum_of_res/(num_res + lambda)

		11. Learning rate is 'eta'

		12. Use the learning rate to calulate the next prediction:

		For each leaf:
			
			Calculate the following
			
				new_prediction = initial_prediction + (eta * output value) 

			Take the new prediction and create a new tree; i.e., go to Step 1 with the new prediction.









Feb 14th, 2021:




Feb 10th, 2021:

    ToDo: BOUGHT 4 DAY: ['SINO', 'WWR', 'CBAT', 'IDEX', 'FUTU', 'RIOT', 'FUV', 'PPSI']

    ToDo: Need to rate 3 and 4 day predictions

Feb 9th, 2021:

    TODO: Added some FIXMEs. Fix the FIXMEs.

    Got these predictions for today: 

    "2021-02-09 process complete: 1 day prediction: ['AXTI', 'AQMS', 'PSNL', 'FLUX', 'KURA', 'TPIC', 'BIDU', 'FREQ']", "2021-02-09 process complete: 3 day prediction: ['OEG']", 
	"2021-02-09 process complete: 4 day prediction: ['SINO', 'WWR', 'CBAT', 'IDEX', 'FUTU', 'RIOT', 'FUV', 'PPSI']"

	2 and 5 day did not work.

